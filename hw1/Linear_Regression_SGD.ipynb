{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Stochastic Gradient Descent\n",
    "\n",
    "Write a program which computes the solution using stochastic gradient descent.  You may use a minibatch size of 5 data points. For convergence, remember to decrease the learning rate over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 50\n",
      "Features, X:  (50, 4)\n",
      "Response, Y:  (50,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "data = np.genfromtxt('linear.csv', delimiter=',')\n",
    "\n",
    "learn_rate = .1\n",
    "num_samples = data.shape[0]\n",
    "print(\"No. of samples:\", num_samples)\n",
    "\n",
    "np.random.shuffle(data)\n",
    "X = data[:,1:]\n",
    "Y = data[:,0]\n",
    "\n",
    "print(\"Features, X: \", X.shape)\n",
    "print(\"Response, Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to shuffle and sample without replacement. \n",
    "\n",
    "Define the function `get_minibatch` to extract a mini batch of data.\n",
    "\n",
    "`get_minibatch_grad` will predict values `y_pred` for the current theta, compute the error, then calculate the gradient of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:  -22.086336174786794\n",
      "LOSS:  -16.64583958102239\n",
      "LOSS:  -13.048554226175057\n",
      "LOSS:  -10.603352742600666\n",
      "LOSS:  -8.884668993644203\n",
      "LOSS:  -7.629896922225029\n",
      "LOSS:  -6.6764781950948136\n",
      "LOSS:  -5.9233217982606865\n",
      "LOSS:  -5.3071411586933275\n",
      "LOSS:  -4.787937298457436\n",
      "LOSS:  -4.340089787358052\n",
      "LOSS:  -3.946886339541239\n",
      "LOSS:  -3.597161203722474\n",
      "LOSS:  -3.2832270286327687\n",
      "LOSS:  -2.9996003191149363\n",
      "LOSS:  -2.7422139770341363\n",
      "LOSS:  -2.5079289750074234\n",
      "LOSS:  -2.294229893822188\n",
      "LOSS:  -2.099033616175349\n",
      "LOSS:  -1.9205677914008132\n",
      "LOSS:  -1.7572924388954232\n",
      "LOSS:  -1.6078483314525334\n",
      "LOSS:  -1.4710221006901048\n",
      "LOSS:  -1.3457218721478865\n",
      "LOSS:  -1.2309596096532296\n",
      "LOSS:  -1.1258378048641309\n",
      "LOSS:  -1.0295390426337725\n",
      "LOSS:  -0.9413175231275789\n",
      "LOSS:  -0.8604919605850676\n",
      "LOSS:  -0.786439487880637\n",
      "LOSS:  -0.7185903256555486\n",
      "LOSS:  -0.656423055465091\n",
      "LOSS:  -0.5994603869423641\n",
      "LOSS:  -0.5472653409770368\n",
      "LOSS:  -0.4994377914371539\n",
      "LOSS:  -0.45561132138045324\n",
      "LOSS:  -0.4154503586875642\n",
      "LOSS:  -0.3786475622508742\n",
      "LOSS:  -0.3449214342868774\n",
      "LOSS:  -0.31401413763526115\n",
      "LOSS:  -0.2856894994549339\n",
      "LOSS:  -0.2597311847700391\n",
      "LOSS:  -0.23594102501078684\n",
      "LOSS:  -0.21413748813276612\n",
      "LOSS:  -0.19415427814754993\n",
      "LOSS:  -0.1758390529986812\n",
      "LOSS:  -0.15905225069888018\n",
      "LOSS:  -0.14366601452679664\n",
      "LOSS:  -0.12956320887874437\n",
      "LOSS:  -0.11663651809434455\n",
      "LOSS:  -0.10478762123279117\n",
      "LOSS:  -0.09392643637597367\n",
      "LOSS:  -0.08397042858168563\n",
      "LOSS:  -0.07484397610968006\n",
      "LOSS:  -0.06647778999978869\n",
      "LOSS:  -0.058808382498485336\n",
      "LOSS:  -0.05177758021204361\n",
      "LOSS:  -0.045332078213251636\n",
      "LOSS:  -0.03942303164807869\n",
      "LOSS:  -0.0340056816806995\n",
      "LOSS:  -0.029039012882729087\n",
      "LOSS:  -0.024485439416942714\n",
      "LOSS:  -0.02031051758984567\n",
      "LOSS:  -0.016482682552094896\n",
      "LOSS:  -0.012973007113528213\n",
      "LOSS:  -0.009754980811028314\n",
      "LOSS:  -0.006804307524630982\n",
      "LOSS:  -0.004098720081109264\n",
      "LOSS:  -0.0016178104158619867\n",
      "LOSS:  0.0006571260154834114\n",
      "LOSS:  0.0027432327740891773\n",
      "LOSS:  0.0046562230208477716\n",
      "LOSS:  0.00641049915004387\n",
      "LOSS:  0.008019262393933413\n",
      "LOSS:  0.009494613240915772\n",
      "LOSS:  0.010847643439036337\n",
      "LOSS:  0.012088520291459893\n",
      "LOSS:  0.013226563891257904\n",
      "LOSS:  0.014270317888235063\n",
      "LOSS:  0.015227614330678707\n",
      "LOSS:  0.016105633079291727\n",
      "LOSS:  0.01691095624866742\n",
      "LOSS:  0.017649618093462206\n",
      "LOSS:  0.01832715072128227\n",
      "LOSS:  0.018948625982202007\n",
      "LOSS:  0.0195186938554771\n",
      "LOSS:  0.02004161762699034\n",
      "LOSS:  0.020521306126389638\n",
      "LOSS:  0.02096134327027778\n",
      "LOSS:  0.02136501513704027\n",
      "LOSS:  0.021735334780115856\n",
      "LOSS:  0.022075064968983205\n",
      "LOSS:  0.02238673903141123\n",
      "LOSS:  0.02267267995579979\n",
      "LOSS:  0.02293501789927991\n",
      "LOSS:  0.023175706234917572\n",
      "LOSS:  0.023396536260180668\n",
      "LOSS:  0.023599150678638284\n",
      "LOSS:  0.023785055957506563\n",
      "LOSS:  0.023955633654892297\n",
      "LOSS:  0.024112150802980806\n",
      "LOSS:  0.024255769425913756\n",
      "LOSS:  0.024387555264696683\n",
      "LOSS:  0.024508485775403422\n",
      "LOSS:  0.024619457461280192\n",
      "LOSS:  0.024721292594398626\n",
      "LOSS:  0.024814745377779026\n",
      "LOSS:  0.024900507594743618\n",
      "LOSS:  0.024979213788154677\n",
      "LOSS:  0.025051446008887326\n",
      "LOSS:  0.025117738169350336\n",
      "LOSS:  0.025178580035030918\n",
      "LOSS:  0.025234420884246402\n",
      "LOSS:  0.025285672863714984\n",
      "LOSS:  0.02533271406531078\n",
      "LOSS:  0.025375891347254678\n",
      "LOSS:  0.025415522920950673\n",
      "LOSS:  0.02545190072305711\n",
      "LOSS:  0.02548529259062743\n",
      "LOSS:  0.025515944255725776\n",
      "LOSS:  0.02554408117451955\n",
      "LOSS:  0.025569910204668825\n",
      "LOSS:  0.02559362114354239\n",
      "LOSS:  0.025615388138923778\n",
      "LOSS:  0.025635370982747763\n",
      "LOSS:  0.025653716297613118\n",
      "LOSS:  0.025670558624979703\n",
      "LOSS:  0.025686021423197994\n",
      "LOSS:  0.025700217982920295\n",
      "LOSS:  0.02571325226663658\n",
      "LOSS:  0.02572521967877245\n",
      "LOSS:  0.02573620777203042\n",
      "LOSS:  0.025746296895252554\n",
      "LOSS:  0.025755560787731325\n",
      "LOSS:  0.02576406712432552\n",
      "LOSS:  0.025771878015525773\n",
      "LOSS:  0.025779050466175204\n",
      "LOSS:  0.025785636796233594\n",
      "LOSS:  0.025791685026865786\n",
      "LOSS:  0.0257972392345215\n",
      "LOSS:  0.025802339875846715\n",
      "LOSS:  0.025807024085753673\n",
      "LOSS:  0.025811325950844954\n",
      "LOSS:  0.02581527676032095\n",
      "LOSS:  0.025818905236133362\n",
      "LOSS:  0.025822237744207832\n",
      "LOSS:  0.025825298488191165\n",
      "LOSS:  0.025828109687248524\n",
      "LOSS:  0.02583069173920659\n",
      "LOSS:  0.025833063370242083\n",
      "LOSS:  0.025835241772214825\n",
      "LOSS:  0.025837242728745515\n",
      "LOSS:  0.025839080730842658\n",
      "LOSS:  0.025840769083055982\n",
      "LOSS:  0.02584232000085688\n",
      "LOSS:  0.02584374470002947\n",
      "LOSS:  0.02584505347869989\n",
      "LOSS:  0.025846255792608033\n",
      "LOSS:  0.02584736032419123\n",
      "LOSS:  0.025848375045998143\n",
      "LOSS:  0.025849307278874214\n"
     ]
    }
   ],
   "source": [
    "def get_minibatch(data, start_index, batch_size):\n",
    "    X = data[:,1:]\n",
    "    Y = data[:,0]\n",
    "    X_mini = X[start_index:start_index+batch_size]\n",
    "    Y_mini = Y[start_index:start_index+batch_size]\n",
    "    return X_mini, Y_mini\n",
    "\n",
    "def get_minibatch_grad(minibatch, theta):\n",
    "    minibatch_X = minibatch[0] # 5 samples, 4 features each incl. 1 constant feature\n",
    "    minibatch_Y = minibatch[1]\n",
    "    y_pred = minibatch_X.dot(theta)\n",
    "    err = minibatch_Y - y_pred.squeeze()\n",
    "    grad = -minibatch_X.T.dot(err) / minibatch[0].shape[0]\n",
    "    \n",
    "    # Prints for visualization, if I ever need them again\n",
    "    # print(\"THE X FEATURES:\" ,minibatch_X.shape, minibatch_X)\n",
    "    # print(\"THE ACTUAL Y LABELS:\" ,minibatch_Y.shape, minibatch_Y)\n",
    "    # print(\"PREDICTED Y VALS FOR THIS 5-BATCH OF X: \", y_pred.shape, y_pred)\n",
    "    # print(\"PREDICTED Y SQUEEZED: \", y_pred.squeeze().shape, y_pred.squeeze())\n",
    "    # print(\"ERR FOR EACH OF 5 SAMPLES: \", err)\n",
    "    # print(\"GRAD: \", grad.shape, grad)\n",
    "    \n",
    "    return grad\n",
    "    \n",
    "\n",
    "def sgd(theta_init, learn_rate_init, batch_size):\n",
    "    theta = theta_init\n",
    "    learn_rate = learn_rate_init\n",
    "    loss = 0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        minibatch = get_minibatch(data, i, batch_size)\n",
    "        batch_grad = get_minibatch_grad(minibatch, theta)\n",
    "        theta = theta.squeeze() - learn_rate * batch_grad\n",
    "        learn_rate = learn_rate/(1+i)\n",
    "        loss = (1/batch_size)*np.sum(Y-X.dot(theta))\n",
    "        \n",
    "        # Prints for visualization\n",
    "        # print(\"BATCHGRAD\", batch_grad.shape, batch_grad)\n",
    "        # print(\"THETA OLD\", theta.shape, theta)\n",
    "        # print(\"THETA NEW\", theta.shape, theta)\n",
    "        # print(\"LEARN RATE\", learn_rate)\n",
    "    return loss, theta\n",
    "        \n",
    "theta_init = np.ones((1,4))\n",
    "prev_loss = sys.maxsize\n",
    "theta = theta_init\n",
    "losses = [] # for plotting\n",
    "iteration = 0 # for plotting\n",
    "\n",
    "while True:\n",
    "    iteration += 1\n",
    "    loss, theta = sgd(theta.T ,learn_rate,5)\n",
    "    print(\"LOSS: \", loss)\n",
    "    losses.append(loss)\n",
    "    loss_diff = abs(prev_loss - loss)\n",
    "    if loss_diff < 0.000001:\n",
    "        break\n",
    "    prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57607184  1.30261501 -0.02598882 -1.87047472]\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(np.arange(iteration), losses, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
